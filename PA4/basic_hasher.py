from collections import defaultdict, Counter
import pickle as pickle
from os.path import join, exists, splitext
import time
import os
import sys
import numpy as np
import itertools
from scipy.spatial import distance
sys.path.insert(0, os.path.abspath(".."))
sys.path.insert(0, os.path.abspath("../.."))
from BIOINFO_M260B.helpers import read_reads


def read_reference(ref_fn):
    f = open(ref_fn, 'r')
    first_line = True
    out_refs = []
    for line in f:
        if first_line:
            first_line = False
            continue
        line = line.strip()
        out_refs.append(line)
    output_reference = ''
    snp_counter = 0
    snp_locations = []
    for i in range(len(out_refs[0])):
        if out_refs[0][i] == out_refs[1][i] == out_refs[2][i] == out_refs[3][i]:
            output_reference += out_refs[0][i]
        else:
            count = Counter()
            snp_counter += 1
            snp_locations.append(i)
            for j in range(len(out_refs)):
                count.update(out_refs[j][i])
            output_reference += count.most_common(1)[0][0]
    print(snp_counter, snp_locations)
    return output_reference, snp_counter, snp_locations
    
    
def pretty_print_aligned_reads_with_ref(genome_oriented_reads, read_alignments, ref, read_length=50,
                                        line_length=100, read_sep=100, buffer=30):
    """
    :param genome_oriented_reads: oriented reads generated by an alignment algorithm
    :param read_alignments: alignments generated from an alignment algorithm
    :param ref: reference generated by read_ref
    :return: Returns nothing, but prints the reads aligned to the genome to
     show you what pileup actually *LOOKS* like. You should be able to call SNPs
     by eyeballing the output. However, there are some reads that will not align.
     In the future you'll want to re-check why these reads aren't aligning--the cause
     is usually a structural variation, like an insertion or deletion.
    """
    output_str = ''
    good_alignments = [True if x != -1 else False for x in read_alignments]
    # There should be read_length + x (90 < x < 110) p between the reads, and we give a little
    # extra space in case there's been a deletion or insertion.  Depending on the type of
    # deletions/insertions

    best_reads = [genome_oriented_reads[i] for i in range(len(good_alignments))
                  if good_alignments[i]]
    # Remove the reads that do not have a good alignment, or a good reverse alignment.
    best_alignments = [read_alignments[i] for i in range(len(read_alignments))
                       if good_alignments[i]]
    # Take their corresponding alignments
#    aligned_reads = [best_reads[i][0] + '.' * (best_alignments[i][1] - best_alignments[i][0] - read_length)
#                     + best_reads[i][1] for i in range(len(best_reads))]
    aligned_reads = [best_reads[i] for i in range(len(best_reads))]
    # This turns the reads into strings oriented towards the genome.
    # We get the first read, followed by the correct number of dots to join the first and second reads,
    # and then the second read.

    first_alignment = [x for x in best_alignments]
    alignment_indices = np.argsort(first_alignment)
    sorted_reads = np.array([aligned_reads[i] for i in alignment_indices])
    sorted_alignments = np.array([best_alignments[i] for i in alignment_indices])
    
#    print("sort_read: ", sorted_reads)
#    print("sort_align: ", sorted_alignments)
    
    # You don't need to worry too much about how the code block below works--its job is to make it so
    # that a read that starts printing in the third row will continue printing in the third row of the
    # next set of lines.
    active_reads = []
    output_str += '\n\n' + '-' * (line_length + 6) + '\n\n'
    read_indices = np.array([sorted_alignments[j] // line_length for j in range(len(sorted_alignments))])
#    print("read_indices: ", read_indices)
    
    for i in range(len(ref) // line_length):
        next_ref = ref[i * line_length: (i + 1) * line_length]
        read_mask = (read_indices == i)
        new_alignments = sorted_alignments[read_mask]
        new_reads = sorted_reads[read_mask]
        space_amounts = [_ % line_length for _ in new_alignments]
        new_reads_with_spaces = [' ' * space_amounts[j] + new_reads[j] for j in range(len(new_reads))]
        empty_active_read_indices = [index for index in range(len(active_reads)) if active_reads[index] == '']
        for j in range(min(len(new_reads_with_spaces), len(empty_active_read_indices))):
            active_reads[empty_active_read_indices[j]] = new_reads_with_spaces[j]
            
        if len(new_reads_with_spaces) > len(empty_active_read_indices):
            active_reads += new_reads_with_spaces[len(empty_active_read_indices):]
        printed_reads = ['Read: ' + read[:line_length] for read in active_reads]
#        print(printed_reads)
        active_reads = [read[line_length:] for read in active_reads]
        while len(active_reads) > 0:
            last_thing = active_reads.pop()
            if last_thing != '':
                active_reads.append(last_thing)
                break
        output_lines = ['Ref:  ' + next_ref] + printed_reads
        output_str += 'Reference index: ' + str(i * line_length) + \
                      '\n' + '\n'.join(output_lines) + '\n\n' + '-' * (line_length + 6) + '\n\n'
    # print output_str
    return output_str
    
    
def hash_end(end, genome_ht):
    """
    Uses hashing to identify the set of locations spanned by
    a read.

    :param end: A single end of a read
    :param genome_ht: A hash of the genome with uniform key length
    :return:
    """
    key_length = len(list(genome_ht.keys())[0])
    end_pieces = [end[i * key_length: (i + 1) * key_length]
                  for i in range(len(end) // key_length)]

    hashed_read_locations = [genome_ht[read_piece]
                             for read_piece in end_pieces]
    start_positions = [[x - i * key_length for x in hashed_read_locations[i]]
                       for i in range(len(hashed_read_locations))]
    start_counter = Counter()

    for position_list in start_positions:
        start_counter.update(position_list)
    
    if not start_counter:
        return -1, 0
    else:
        best_alignment_location, best_alignment_count = \
            start_counter.most_common(1)[0]

    if best_alignment_count < 2:
        return -1, best_alignment_count
    else:
        return best_alignment_location, best_alignment_count


def hash_read(read, genome_ht):
    """
    Uses hashing to identify the set of locations spanned by
    a read.

    :param read: A single read
    :param genome_ht: A hash of the genome with uniform key length
    :return:
    """
    
    # single-end read in the viral genome project
    oriented_read = read[0]
#    oriented_reads = [(read[0][::i], read[1][::j]) for i, j in ((1, -1), (-1, 1))]
    ## Either one end is forward and the other end is reversed, or vice versa.

    best_score = -1
    best_alignment_locations = -1
    best_oriented_read = ''
#    for oriented_read in oriented_reads:
#    hash_results = [hash_end(_, genome_ht) for _ in oriented_read]
#    hash_locations = [_[0] for _ in hash_results]
#    hash_score = sum([_[1] for _ in hash_results])
    hash_results = hash_end(oriented_read, genome_ht)
    hash_locations = hash_results[0]
    hash_score = hash_results[1]
    if hash_score > best_score:
        best_alignment_locations = hash_locations
        best_oriented_read = oriented_read
    return best_oriented_read, best_alignment_locations


def make_genome_hash(reference, key_length):
    """

    :param reference: The reference as a string stored
    :param key_length: The length of keys to use.
    :return:
    """
    genome_hash = defaultdict(list)
    for i in range(len(reference) - key_length):
        ref_piece = reference[i: i + key_length]
        genome_hash[ref_piece].append(i)
    return genome_hash


def build_hash_and_pickle(ref_fn, key_length, force_rebuild=False):
    reference_hash_pkl_fn = '{}_hash_keylength_{}.pkl'.format(splitext(ref_fn)[0], key_length)
    if exists(reference_hash_pkl_fn) and not force_rebuild:
        ref_genome_hash = pickle.load(open(reference_hash_pkl_fn, 'rb'))
        if len(list(ref_genome_hash.keys())[0]) == key_length:
            return ref_genome_hash
        else:
            pass
    else:
        pass
    reference = read_reference(ref_fn)[0]
    ref_genome_hash = make_genome_hash(reference, key_length)
    pickle.dump(ref_genome_hash, open(reference_hash_pkl_fn, 'wb'))
    return ref_genome_hash


def hashing_algorithm(paired_end_reads, genome_ht):
    """

    :param paired_end_reads:
    :param genome_ht:
    :return:
    """
    alignments = []
    genome_aligned_reads = []
    count = 0
    start = time.clock()

    for read in paired_end_reads:
        alignment, genome_aligned_read = hash_read(read, genome_ht)
        alignments.append(alignment)
        genome_aligned_reads.append(genome_aligned_read)
        count += 1
        if count % 1000 == 0:
            time_passed = (time.clock()-start)/60
            print ('{} reads aligned'.format(count), 'in {:.3} minutes'.format(time_passed))
            remaining_time = time_passed/count*(len(paired_end_reads)-count)
            print ('Approximately {:.3} minutes remaining'.format(remaining_time))
    return alignments, genome_aligned_reads
    

def generate_snp_frequencies(aligned_fn, snp_locs):
    """
    :param aligned_fn: The filename of the saved output of the basic aligner
    :return: SNPs (the called SNPs for uploading to the herokuapp server)
             output_lines (the reference, reads, consensus string, and diff string to be printed)
    """
    line_count = 0
    lines_to_process = []
    snp_indices = [i // 100 * 100 for i in snp_locs]
    print(snp_indices)
    snp_freq = []
    with open(aligned_fn, 'r') as input_file:
        for line in input_file:
            line_count += 1
            line = line.strip()
            if line_count <= 4 or line == '':  # The first 4 lines need to be skipped
                continue
            if len(line) > 0 and all(x == '-' for x in line):  # The different pieces of the genome are set off
                # with lines of all dashes '--------'
                line_ct = 0
                consensus_lines = []
                line_index = 0
                for line in lines_to_process:
                    line_ct += 1
                    if line_ct == 1:  # The first line contains the position in the reference where the reads start.
                        raw_index = line.split(':')[1]
                        line_index = int(raw_index)
                        # check if it is the location of SNPs
                        if line_index not in snp_indices:
                            break
                    else:
                        consensus_lines.append(line[6:])
                if (len(consensus_lines) > 0):
#                    for i in consensus_lines:
#                        print (i)
                    ref = consensus_lines[0]
                    aligned_reads = consensus_lines[1:]
                    snp_indexes = [line_index // 100 == i // 100 for i in snp_locs]
                    snp_index = [snp_locs[x] % 100 for x in range(len(snp_locs)) if snp_indexes[x]][0]
                    print(snp_index)
                    match = 0
                    mismatch = 0
                    for i in aligned_reads:
                        try:
                            if (i[snp_index] == ref[snp_index]):
                                match += 1 
                            else:
                                mismatch += 1
                        except:
                            continue
                            
                    snp_freq.append(mismatch / (match+mismatch))
                else:
                    lines_to_process = []
                    continue
                
                lines_to_process = []
            else:
                lines_to_process.append(line)
    print(snp_freq)
    return snp_freq
    
    
def generate_test_SNPs(snp_freqs, n_strains=4, coverage=100, set_seed=False, n_snps=7):
    ## We're going to assume there is overlap between
    ## Strains for each SNP.
    
    if set_seed: np.random.seed(96000)
    
#    assert sum(snp_freqs) == 1
    
    powerset_iterable = itertools.chain.from_iterable(itertools.combinations(range(n_strains), r)
                                             for r in range(2, n_strains))
    
    frozen_powerset = [index_tuple for index_tuple in powerset_iterable]
    
    while True:
        these_indices = np.random.choice(frozen_powerset, n_snps)
        output_strains = [[] for i in range(n_strains)]
        for index_list in these_indices:
            for i in range(len(output_strains)):
                if i in index_list:
                    output_strains[i].append(1)
                else:
                    output_strains[i].append(0)
                    
        output_strains = [tuple(strain) for strain in output_strains]
        n_unique_strains = len(set(output_strains))
        # test if it satisfies a few conditions 
        # if it does, we return
        # otherwise; loop back and try again
        strain_matrix = np.array(output_strains)
        snp_freqs = np.array(snp_freqs)
#        print(snp_freqs)
        strain_freqs = np.linalg.lstsq(strain_matrix.T, snp_freqs)
        bools = strain_freqs[0] > 0
#        print(snp_freqs[0])
        if n_unique_strains == n_strains and bools.all() and abs(sum(strain_freqs[0])-1) < 0.01 \
            and distance.euclidean(strain_matrix.T.dot(strain_freqs[0]), snp_freqs) < 0.02:
                return strain_freqs[0]
#        if n_unique_strains == n_strains:
#            break
#     for freq, strain in zip(snp_freqs, output_strains):
#         print freq, strain
#
#    sampled_strains = np.random.choice(range(n_strains),
#                                       size=coverage*n_strains,
#                                       p=snp_freqs)
#    
#    output_snp_counts = defaultdict(list)
#    
#    for strain_index in sampled_strains:
#        snp_index = np.random.choice(range(10))
#        snp_value = output_strains[strain_index][snp_index]
#        output_snp_counts[snp_index].append(snp_value)
#    
##     print sampled_strains
#    
#    output_snp_freqs = {k: float(sum(v))/len(v) for k,v in 
#                       output_snp_counts.items()}
#    
##     for k, v in output_snp_counts.items(): print (k, v[:10])
##     for k, v in output_snp_freqs.items(): print (k, v)
#    return output_snp_counts, output_strains

#
#def get_freqs_from_strains_and_counts(input_snp_counts, input_strains):
##     print input_snp_count
#    snp_freqs = []
#    for k in range(10):
#        raw_snps = input_snp_counts[k]
#        snp_freq = float(sum(raw_snps))/len(raw_snps)
#        snp_freqs.append(snp_freq)
#    
#    snp_freqs = np.array(snp_freqs)
#    
#    strain_matrix = np.array(input_strains)
#    print (strain_matrix)
#    
#    snp_freqs = np.linalg.lstsq(strain_matrix.T, snp_freqs)
#    for freq, strain in zip(snp_freqs[0], input_strains):
#        print (freq, strain)
#        
#    return snp_freqs[0]
    
    
if __name__ == "__main__":
    genome_name = 'hw4_W_0'
    input_folder = '../data/{}'.format(genome_name) + '/'
    chr_name = '{}_chr_1'.format(genome_name)
    reads_fn_end = 'reads_{}.txt'.format(chr_name)
    reads_fn = join(input_folder, reads_fn_end)
    ref_fn_end = 'ref_{}.txt'.format(chr_name)
    ref_fn = join(input_folder, ref_fn_end)
    key_length = 10
    start = time.clock()
    reads = read_reads(reads_fn)
    # If you want to speed it up, cut down the number of reads by
    # changing the line to reads = read_reads(reads_fn)[:<x>] where <x>
    # is the number of reads you want to work with.
    genome_hash_table = build_hash_and_pickle(ref_fn, key_length)
    ref, snp_counts, snp_locs = read_reference(ref_fn)
    genome_aligned_reads, alignments = hashing_algorithm(reads, genome_hash_table)
    # print genome_aligned_reads
    # print alignments
    output_str = pretty_print_aligned_reads_with_ref(genome_aligned_reads, alignments, ref)
    output_fn = join(input_folder, 'aligned_10_{}.txt'.format(chr_name))
    with(open(output_fn, 'w')) as output_file:
        output_file.write(output_str)
#    print (output_str[:5000])
    snq_freq = generate_snp_frequencies(output_fn, snp_locs)
    strain_freq = generate_test_SNPs(snp_freqs = snq_freq, coverage=100, n_snps=snp_counts)
    print(strain_freq)
    
    
#    input_snp_counts, input_strains = generate_test_SNPs(snp_freqs = snq_freq, coverage=100, n_snps=snp_counts)
#    for k, v in input_snp_counts.items(): print (k, v[:10])
#    snp_freqs = get_freqs_from_strains_and_counts(input_snp_counts, input_strains)
